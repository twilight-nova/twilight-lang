# Twilight Compiler Frontend Specification v1.0

## 1. Introduction

### 1.1 Purpose

This document specifies the architecture, algorithms, and responsibilities of the Twilight compiler (`twilightc`) frontend. The frontend is responsible for processing Twilight source code files (`.twl`), handling different Twilight Syntax Modes (TSM), parsing the code into an Abstract Syntax Tree (AST), performing initial semantic analysis (name resolution, basic type checking), and lowering the representation into High-Level Twilight Intermediate Representation (TIR-HL) suitable for the compiler's middle end.

### 1.2 Scope

This specification details the stages within the frontend, including syntax mode detection, lexing, parsing, AST structure principles, AST transformation (for non-native syntax modes), initial semantic checks, and TIR-HL generation. It does not cover detailed middle-end analyses or backend code generation.

## 2. Design Philosophy

* **Multi-Syntax Support (TSM):** Handle different input syntaxes (initially `twilight-native` and `solidity`) via distinct parsing paths that converge to a common internal representation (Twilight AST and subsequently TIR-HL).
* **Error Reporting:** Generate clear, precise, and actionable error messages for syntax and early semantic issues, leveraging detailed source location tracking (spans) attached to all intermediate representations.
* **Robust Parsing:** Utilize established parsing techniques and potentially leverage existing, mature parser libraries (especially for `solidity` mode) to ensure grammar conformance and handle syntax errors gracefully.
* **Foundation for Middle End:** Produce a well-formed, annotated TIR-HL representation that contains sufficient information (resolved names, basic type information, source locations, annotations) for subsequent, more complex analysis and optimization passes in the middle end.

## 3. Frontend Pipeline Stages

The frontend processes source code through the following conceptual stages:

[Source Files (.twl)]
|
V
[1. Input Processing & Syntax Mode Detection] --> SyntaxMode
|
+----(IF Native)----> [2a. Lexing (Native)] -> [3a. Parsing (Native)] --------> [4. Twilight AST]
|
+----(IF Solidity)----> [2b. Lexing (Solidity)] -> [3b. Parsing (Solidity)] -> [Solidity AST] -> [3c. AST Transform] -> [4. Twilight AST]
|
V
[5. AST -> TIR-HL Lowering]
(Name Resolution, Basic Type Check, Annotation Processing, TIR-HL Gen)
|
V
[TIR-HL Output (to Middle End)]

### 3.1 Input Processing & Syntax Mode Detection

* **Input:** A set of `.twl` source files constituting a compilation unit (crate).
* **Process:**
    1.  Read source file content into memory, associating it with a unique File ID.
    2.  Examine the beginning of each file for a syntax directive: `#![syntax = "MODE"]` (where `MODE` is e.g., `"native"`, `"solidity"`).
    3.  Determine the `SyntaxMode` for the file. If the directive is absent, default to `twilight-native`.
* **Output:** Internal representation mapping File IDs to their source text and detected `SyntaxMode`.

### 3.2 Lexing (Tokenization)

* **Input:** Source text and its `SyntaxMode`.
* **Process:**
    1.  Select the appropriate lexer based on `SyntaxMode`.
        * `twilight-native`: Use a lexer implementation (e.g., generated by `logos`) conforming to the token definitions in `specs/language/02-grammar.md`.
        * `solidity`: Use the lexer from the chosen Solidity parsing library (e.g., Solang's lexer).
    2.  Convert the source text into a stream of tokens, each with a `TokenKind` and a `Span` (source location including File ID and byte offsets).
    3.  Handle lexical errors (e.g., invalid characters) by generating diagnostics.
    4.  Ignore whitespace and non-doc comments. Associate `DOC_COMMENT` tokens with subsequent items.
* **Output:** A sequence of `Token`s for the file and a list of any lexical `Diagnostic`s.

### 3.3 Parsing (AST Generation)

* **Input:** Token stream and `SyntaxMode`.
* **Process:** Select the appropriate parser based on `SyntaxMode`.

#### 3.3.1 `twilight-native` Parsing

1.  Invoke a parser implementation (e.g., generated by `LALRPOP`) based on the EBNF grammar defined in `specs/language/02-grammar.md`.
2.  Consume the token stream.
3.  Construct the canonical Twilight Abstract Syntax Tree (AST), as defined in Section 3.4, upon successful application of grammar rules.
4.  Report syntax errors using precise `Span` information, attempting basic error recovery (e.g., synchronizing on `;` or `}`) to find multiple errors per file.
5.  Attach documentation comments to corresponding AST nodes.
* **Output:** A `TwilightAst::CompilationUnit` and a list of syntax `Diagnostic`s.

#### 3.3.2 `solidity` Mode Parsing & AST Transformation

1.  Invoke the parser from the chosen Solidity parsing library (e.g., Solang's parser), consuming the token stream from the Solidity lexer.
2.  This produces the library's native representation of the Solidity AST (e.g., `SolangAst::SourceUnit`). Handle parsing errors reported by the library.
3.  **AST Transformation:** Traverse the Solidity-specific AST. For each relevant node (contract, function, state variable, expression, etc.), construct the equivalent canonical Twilight AST node according to the mapping rules defined in `specs/language/02-grammar.md`, Section 4.
    * This involves translating types, visibility, modifiers (via code inlining), globals (`msg.sender` -> `ctx.sender()`), events (`indexed` -> `#[topic]`), etc.
    * Preserve source location information as accurately as possible during the transformation.
4.  **Unsupported Feature Handling:** Detect Solidity features explicitly marked as unsupported (e.g., `assembly`, `delegatecall`). Generate compile-time error `Diagnostic`s indicating the unsupported feature and its location; do not generate Twilight AST nodes for these features.
* **Output:** A `TwilightAst::CompilationUnit` (conforming to the canonical Twilight AST structure) and a list of syntax or transformation `Diagnostic`s.

### 3.4 Abstract Syntax Tree (AST) Structure

* **Purpose:** Represents the syntactic structure of the code after parsing (and potential transformation), serving as the input for further semantic analysis.
* **Node Definition:** Defined by Rust `struct`s and `enum`s, generally corresponding to non-terminal symbols in the `twilight-native` grammar. Key node types include `CompilationUnit`, `Item` (FunctionDef, StructDef, ContractDef, etc.), `Statement` (LetStmt, ExprStmt), `Expression` (Literal, BinaryOp, FunctionCall, IfExpr, etc.), `Type`, `Pattern`.
* **Source Spans:** Every AST node **must** contain `Span` information accurately mapping it back to the original source code location (file, start/end offsets or line/column).
* **Common Representation:** Both `twilight-native` and `solidity` mode parsers ultimately produce ASTs conforming to this *same* canonical Twilight AST definition.

*(Note: Detailed AST node definitions were specified previously, derived from `twilightc.txt` Part 2 / Spec 3.1 in the blueprint sources).*

### 3.5 AST to TIR-HL Lowering

* **Input:** The canonical Twilight AST (`TwilightAst::CompilationUnit`) for all files in the crate, potentially module structure information, and dependency metadata.
* **Process:** Traverses the AST performs initial semantic analysis, and generates the High-Level Twilight Intermediate Representation (TIR-HL). This involves several sub-stages:

    1.  **Name Resolution:**
        * Builds symbol tables for scopes (modules, contracts, functions, blocks).
        * Resolves all identifiers used in expressions, types, and paths to their corresponding definitions according to language scoping rules (local, module, imports, prelude) and visibility (`pub`, `pub(crate)`).
        * Reports errors for undefined or inaccessible names. Links identifier usages to their definitions (e.g., via IDs or pointers).
    2.  **Basic Type Checking & Inference:**
        * Performs initial type checking on expressions, assignments, function calls (arity, basic parameter types).
        * Infers types for `val`/`var` bindings where no explicit annotation is given, based on the initializer expression.
        * Reports fundamental type errors detectable at this stage. (Full type checking occurs in the middle end on TIR-SSA). Annotates bindings and potentially expression nodes with resolved basic types.
    3.  **Annotation Processing:**
        * Parses and validates standard annotations (`#[reads]`, `#[writes]`, `#[payable]`, `#[topic]`, `#[verify]`, `#[pipeline]`, etc.).
        * Stores the processed annotation information, preparing it for inclusion as TIR-HL metadata.
    4.  **TIR-HL Generation:**
        * Traverses the semantically analyzed AST.
        * Constructs the TIR-HL representation, mapping AST constructs (functions, statements, expressions) to corresponding TIR-HL elements (functions, basic blocks, instructions).
        * TIR-HL may retain some higher-level structure compared to TIR-SSA (e.g., named field access).
        * Attaches essential metadata collected so far (especially `srcloc`, initial annotation data) to the generated TIR-HL nodes.

### 3.6 Frontend Error Handling & Diagnostics

* **Unified Diagnostics:** All frontend stages (lexing, parsing, AST transformation, lowering) report errors and warnings using a common `Diagnostic` structure.
* **Content:** Diagnostics must include severity (Error, Warning), a clear message, the primary source `Span`, and potentially secondary spans/labels for related code locations.
* **Recovery:** Parsers should implement basic error recovery strategies (e.g., synchronizing on terminators like `;` or `}`) to attempt to find and report multiple syntax errors within a single file. Subsequent analysis stages may skip processing items that had fatal errors in preceding stages but should attempt to continue where possible.

## 4. Output

The primary output of the frontend stage is the High-Level Twilight Intermediate Representation (TIR-HL) for the entire crate, along with a consolidated list of all diagnostics generated during the frontend process. This TIR-HL serves as the input to the middle end.